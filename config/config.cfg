paths{
    data_path: "/media/alex/Data/Master/Thesis/data"
}


core: {
    loguru_verbosity: 3
}

loader: {
    player_paused: true

    dataset_type: "eth"
    only_rgb: false
    // pose_file: "euroc_datasets/V1_01_easy/mav0/state_groundtruth_estimate0/data.csv"
    // pose_file: "euroc_datasets/V1_01_easy/odometry_poses/vo_poses_eth_1.txt"
    // pose_file: "euroc_datasets/V1_01_easy/odometry_poses/alex_slam_eth_v1.txt"
    // pose_timestamp_multiplier: 1
    pose_file: "euroc_datasets/V1_01_easy/odometry_poses/slam_eth.txt"
    pose_timestamp_multiplier: 1e9
    // //paths for each camera
    rgb_path_cam_0: "euroc_datasets/V1_01_easy/mav0/cam0/data"
    rgb_path_cam_1: "euroc_datasets/V1_01_easy/mav0/cam1/data"

    // dataset_type: "icl"
    // only_rgb: false
    // pose_file: "ICL_NUIM/living_room_traj2_frei_png/livingRoom2.gt.freiburg"
    // // //paths for icl-nuim- Even though it's monocular it's easy for debugging now to set both
    // rgb_path_cam_0: "ICL_NUIM/living_room_traj2_frei_png/rgb"
    // rgb_path_cam_1: "ICL_NUIM/living_room_traj2_frei_png/rgb"

    // dataset_type: "nts"
    // only_rgb: false
    // pose_file: "new_tsukuba/new_tsukuba/NTSD-200/groundtruth/camera_track.txt"
    // rgb_path_cam_0: "new_tsukuba/new_tsukuba/NTSD-200/illumination/daylight/left"
    // rgb_path_cam_1: "new_tsukuba/new_tsukuba/NTSD-200/illumination/daylight/right"

    // //remote one
    // dataset_type: "nts"
    // only_rgb: false
    // pose_file: "/home/local/rosu/data/new_tsukuba/NTSD-200/groundtruth/camera_track.txt"
    // rgb_path_cam_0: "/home/local/rosu/data/new_tsukuba/NTSD-200/illumination/daylight/left"
    // rgb_path_cam_1: "/home/local/rosu/data/new_tsukuba/NTSD-200/illumination/daylight/right"


    // //the poses are irrelevant
    // only_rgb: true //ignores poses intrinsics and so on
    // rgb_path_cam_0: "/media/alex/Data/Master/Thesis/data/middelbury/tsukuba_parsed/left"
    // rgb_path_cam_1: "/media/alex/Data/Master/Thesis/data/middelbury/tsukuba_parsed/right"

    nr_cams: 2
    imgs_to_skip: 100
    nr_images_to_read: -1
    pyr_min_lvl: 0
    pyr_max_lvl: 4

    min_starting_depth: 0.1 //for when seeds get created the min and mean are used for intializing the idepthminmax
    mean_starting_depth: 2.0

}


loader_ros: {
    nr_cams: 2
    topic_cam_0: "/img_with_pose_0"
    topic_cam_1: "/img_with_pose_1"
}

//stuff for depth estimation
depth: {
    gl_profiling_enabled: true
    print_stats_enabled: true
    debug_enabled: false
    pattern_file: "pattern_1.png"
    // pattern_file: "pattern_full_cross_3x3.png"
    estimated_seeds_per_keyframe: 400000 //conservative estimate of nr of seeds created per frame
    nr_buffered_keyframes: 2 //nr of keyframes for which we store the seeds
    error_type: 3 //0=BCA, 1=grad_magnitude, 2=original_ugf, 3=sgf

}

//stuff for depth estimation using halide
depth_halide: {
    use_cost_volume_filtering: false
}

visualization:{
    accumulate_meshes: true //acumulates all the local meshes got from each keyframe
    merge_meshes: true //merges all the meshes into one final one, is slow but it's good for saving the final mesh easily
    frustum_scale_multiplier: 0.1
    do_transform_mesh_to_worlGL: true
    tf_worldGL_worldROS_angle: -3.54 // angle in radians to rotate the frame (eg:-0.5*M_PI)
    tf_worldGL_worldROS_axis: "x" // about which axis to rotate: x,y or z
    preload_mesh: true
    // preload_mesh_path: "/media/alex/Data/Master/Thesis/data/euroc_datasets/V1_01_easy/mav0/pointcloud0/data_clean_2.ply"
    preload_mesh_path: "/media/alex/Data/my_papers/renegade_scripts/sparse_cloud/slam_map.ply"
    preload_mesh_subsample_factor: 1 //keep only a fifth of the points in this point cloud
}
